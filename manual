# AI Linux - Detailed Manual

## **Introduction**
AI Linux is a powerful system for log analysis and debugging, leveraging modern AI models like OpenAI GPT-4, LLaMA, and Google Gemini. It consists of multiple components, including a Flask backend, an Electron-based frontend, and WebSocket communication for real-time AI interactions.

---

## **1. System Architecture**
### **1.1 Overview**
The system is designed with a modular approach, consisting of the following components:
- **Frontend (Electron):** Provides the user interface.
- **Backend (Flask):** Handles log processing and AI interactions.
- **AI Models:** Used for log analysis.
- **WebSockets & REST API:** Enable communication between the frontend and backend.
- **Database/Storage (optional):** For saving logs and AI responses.

---

## **2. Backend (Flask) Functionality**
### **2.1 Installation and Setup**
1. Ensure Python 3 is installed.
2. Navigate to the backend directory:
   ```bash
   cd backend
   pip install -r requirements.txt
   ```
3. Start the backend:
   ```bash
   python app.py
   ```

### **2.2 API Endpoints**
The backend provides several API endpoints for handling log analysis and AI interactions:
- `POST /debug` â†’ Receives log text and processes it with AI.
- `GET /logs` â†’ Retrieves saved log files.
- `POST /settings` â†’ Updates configuration settings.

### **2.3 Log Processing Workflow**
1. Logs are received via the `POST /debug` endpoint.
2. The logs are processed and cleaned.
3. AI analysis is performed using selected models.
4. The AI response is returned to the frontend.

### **2.4 AI Model Integration**
The backend interacts with three AI models:
- **OpenAI GPT-4** (via API calls).
- **LLaMA** (local model execution).
- **Google Gemini** (cloud-based processing).

Each AI model has its dedicated function within `ai_model.py` to process input logs and return structured feedback.

---

## **3. Frontend (Electron) Functionality**
### **3.1 Installation and Setup**
1. Ensure Node.js and npm are installed.
2. Navigate to the frontend directory:
   ```bash
   cd frontend
   npm install
   ```
3. Start the frontend:
   ```bash
   npm start
   ```

### **3.2 UI Components**
The Electron frontend consists of multiple views:
- **Log Input Screen:** Allows users to input logs and select AI models.
- **Analysis Results:** Displays AI-generated insights.
- **Settings Panel:** Allows users to configure API keys and model preferences.

### **3.3 Communication with Backend**
The frontend communicates with the backend using:
- **WebSockets** for real-time interaction.
- **REST API Calls** for fetching logs and settings.

---

## **4. WebSocket Communication**
The project uses WebSockets to enable real-time data transfer between the frontend and backend.

### **4.1 WebSocket Events**
- **`toggle-llama`** â†’ Triggers LLaMA model processing.
- **`toggle-log`** â†’ Fetches the latest logs from the backend.
- **`chat-message`** â†’ Sends a user message for AI processing.

---

## **5. Configuration and Settings**
The system includes a settings module for managing AI preferences, API keys, and other configurations.

### **5.1 Modifying Configuration Files**
Configuration is stored in `config.json`. Key settings include:
```json
{
  "openai_api_key": "YOUR_API_KEY",
  "llama_model_path": "/path/to/llama.bin",
  "gemini_api_key": "YOUR_GEMINI_API_KEY",
  "log_level": "info"
}
```

### **5.2 Changing Settings via API**
Users can update settings dynamically using:
```bash
curl -X POST http://localhost:8081/settings -d '{"log_level": "debug"}'
```

---

## **6. Twitch Integration**
A custom Twitch bot allows AI-driven interactions in chat.

### **6.1 Setting up the Twitch Bot**
1. Ensure the `TWITCH_BOT_TOKEN` environment variable is set.
2. Run the bot:
   ```bash
   python frontend/twitchbot.py
   ```
3. Use `!hello` in chat to trigger an AI response.

---

## **7. Deployment Guide**
### **7.1 Running Locally**
For local testing, run both backend and frontend as described in earlier sections.

### **7.2 Running as a Service (Systemd)**
For persistent execution, create a systemd service file:
```ini
[Unit]
Description=AI Linux Backend
After=network.target

[Service]
ExecStart=/usr/bin/python3 /home/user/ailinux/backend/app.py
Restart=always

[Install]
WantedBy=multi-user.target
```
Enable and start the service:
```bash
sudo systemctl enable ailinux.service
sudo systemctl start ailinux.service
```

---

## **8. Debugging and Troubleshooting**
### **8.1 Checking Logs**
To view backend logs:
```bash
tail -f backend/system.log
```
To view frontend logs:
```bash
tail -f frontend/logs/output.log
```

### **8.2 Common Issues**
- **Backend not starting?** Ensure all dependencies are installed.
- **Frontend crashes?** Check Electron version compatibility.
- **AI responses not working?** Ensure API keys are valid and set correctly.

---

## **9. Future Improvements**
- Adding more AI models for better log analysis.
- Implementing user authentication for secure log access.
- Expanding Twitch bot capabilities with more commands.

---

## **10. Conclusion**
AI Linux is a powerful tool that combines log analysis with AI-driven insights. With its modular design, users can integrate it into various debugging workflows while customizing AI interactions.

ðŸš€ Enjoy working with AI Linux!

