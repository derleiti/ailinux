************* Module client.start
client/start.js:1:1: E0001: Parsing failed: 'invalid syntax (client.start, line 1)' (syntax-error)
************* Module client.adjust_hierarchy_with_debugger
client/adjust_hierarchy_with_debugger.py:15:8: E0001: Parsing failed: 'unexpected indent (client.adjust_hierarchy_with_debugger, line 15)' (syntax-error)
************* Module client.frontend.settings
client/frontend/settings.html:8:53: E0001: Parsing failed: 'invalid decimal literal (client.frontend.settings, line 8)' (syntax-error)
************* Module client.frontend.logmanager
client/frontend/logmanager.js:1:1: E0001: Parsing failed: 'invalid syntax (client.frontend.logmanager, line 1)' (syntax-error)
************* Module client.frontend.preload
client/frontend/preload.js:19:43: E0001: Parsing failed: 'unterminated string literal (detected at line 19) (client.frontend.preload, line 19)' (syntax-error)
************* Module client.frontend.importexport
client/frontend/importexport.js:4:49: E0001: Parsing failed: 'unterminated string literal (detected at line 4) (client.frontend.importexport, line 4)' (syntax-error)
************* Module client.frontend.index
client/frontend/index.html:29:44: E0001: Parsing failed: 'invalid decimal literal (client.frontend.index, line 29)' (syntax-error)
************* Module client.frontend.gemini-api-setup
client/frontend/gemini-api-setup.js:1:7: E0001: Parsing failed: 'invalid syntax (client.frontend.gemini-api-setup, line 1)' (syntax-error)
************* Module client.frontend.main
client/frontend/main.js:1:1: E0001: Parsing failed: 'invalid syntax (client.frontend.main, line 1)' (syntax-error)
************* Module client.frontend.log
client/frontend/log.html:1:1: E0001: Parsing failed: 'invalid syntax (client.frontend.log, line 1)' (syntax-error)
************* Module client.frontend.aiineraction
client/frontend/aiineraction.html:9:16: E0001: Parsing failed: 'invalid decimal literal (client.frontend.aiineraction, line 9)' (syntax-error)
************* Module client.frontend.llama
client/frontend/llama.html:9:16: E0001: Parsing failed: 'invalid decimal literal (client.frontend.llama, line 9)' (syntax-error)
************* Module server.backend.data_server
server/backend/data_server.py:254:13: E0001: Parsing failed: 'expected 'except' or 'finally' block (server.backend.data_server, line 254)' (syntax-error)

************* Module client.backend.app
client/backend/app.py:66:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:76:0: C0301: Line too long (102/100) (line-too-long)
client/backend/app.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:112:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:114:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:165:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:169:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:173:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:177:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:271:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/app.py:26:11: W1508: os.getenv default type is builtins.int. Expected str or None. (invalid-envvar-default)
client/backend/app.py:91:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:75:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:76:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:83:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:95:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:115:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:116:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:130:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:131:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:152:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:153:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:180:15: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:174:12: C0415: Import outside toplevel (json) (import-outside-toplevel)
client/backend/app.py:175:17: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
client/backend/app.py:178:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:181:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:207:15: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:185:12: C0415: Import outside toplevel (json) (import-outside-toplevel)
client/backend/app.py:186:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
client/backend/app.py:187:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
client/backend/app.py:208:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:258:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/app.py:259:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:272:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:273:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/app.py:11:0: W0611: Unused send_from_directory imported from flask (unused-import)
************* Module client.backend.huggingface
client/backend/huggingface.py:27:0: C0301: Line too long (102/100) (line-too-long)
client/backend/huggingface.py:53:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:57:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:73:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:76:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:89:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:94:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:99:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:104:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:109:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:114:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:164:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:167:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:170:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:183:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:189:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:192:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:200:35: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:201:34: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:216:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:218:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:223:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:229:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:240:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:261:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:263:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:267:0: C0301: Line too long (114/100) (line-too-long)
client/backend/huggingface.py:274:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:279:35: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:280:34: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:295:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:300:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:304:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:309:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:313:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:322:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:325:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:328:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:346:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:350:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:353:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:361:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:364:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:390:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:393:104: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:393:0: C0301: Line too long (104/100) (line-too-long)
client/backend/huggingface.py:401:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:403:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:408:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:412:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:422:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:425:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:428:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:438:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:444:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:450:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:454:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/huggingface.py:34:0: C0103: Constant name "_model_info_cache" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/huggingface.py:37:0: C0103: Constant name "_cache_last_updated" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/huggingface.py:52:4: W0603: Using the global statement (global-statement)
client/backend/huggingface.py:78:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:61:21: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
client/backend/huggingface.py:65:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:79:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:91:8: C0415: Import outside toplevel (torch) (import-outside-toplevel)
client/backend/huggingface.py:96:8: C0415: Import outside toplevel (transformers) (import-outside-toplevel)
client/backend/huggingface.py:101:8: C0415: Import outside toplevel (huggingface_hub) (import-outside-toplevel)
client/backend/huggingface.py:106:8: C0415: Import outside toplevel (accelerate) (import-outside-toplevel)
client/backend/huggingface.py:111:8: C0415: Import outside toplevel (bitsandbytes) (import-outside-toplevel)
client/backend/huggingface.py:116:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:117:8: W1201: Use lazy % formatting in logging functions (logging-not-lazy)
client/backend/huggingface.py:127:15: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:121:12: C0415: Import outside toplevel (torch) (import-outside-toplevel)
client/backend/huggingface.py:124:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:128:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:96:8: W0611: Unused import transformers (unused-import)
client/backend/huggingface.py:101:8: W0611: Unused import huggingface_hub (unused-import)
client/backend/huggingface.py:106:8: W0611: Unused import accelerate (unused-import)
client/backend/huggingface.py:111:8: W0611: Unused import bitsandbytes (unused-import)
client/backend/huggingface.py:138:8: C0415: Import outside toplevel (torch) (import-outside-toplevel)
client/backend/huggingface.py:139:8: C0415: Import outside toplevel (transformers) (import-outside-toplevel)
client/backend/huggingface.py:138:8: W0611: Unused import torch (unused-import)
client/backend/huggingface.py:139:8: W0611: Unused import transformers (unused-import)
client/backend/huggingface.py:155:4: W0603: Using the global statement (global-statement)
client/backend/huggingface.py:194:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:166:8: C0415: Import outside toplevel (huggingface_hub.HfApi) (import-outside-toplevel)
client/backend/huggingface.py:195:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:242:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:215:8: C0415: Import outside toplevel (huggingface_hub.HfApi) (import-outside-toplevel)
client/backend/huggingface.py:243:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:272:19: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:273:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:330:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:302:8: C0415: Import outside toplevel (torch) (import-outside-toplevel)
client/backend/huggingface.py:303:8: C0415: Import outside toplevel (transformers.pipeline, transformers.AutoTokenizer, transformers.AutoModelForCausalLM) (import-outside-toplevel)
client/backend/huggingface.py:311:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:327:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:331:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:303:8: W0611: Unused AutoTokenizer imported from transformers (unused-import)
client/backend/huggingface.py:303:8: W0611: Unused AutoModelForCausalLM imported from transformers (unused-import)
client/backend/huggingface.py:366:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:352:8: C0415: Import outside toplevel (transformers.AutoTokenizer) (import-outside-toplevel)
client/backend/huggingface.py:355:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:367:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:430:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:410:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:424:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:431:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:437:4: W0602: Using global for '_model_info_cache' but no assignment is done (global-variable-not-assigned)
client/backend/huggingface.py:457:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/huggingface.py:452:13: W1514: Using open without explicitly specifying an encoding (unspecified-encoding)
client/backend/huggingface.py:456:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:458:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/huggingface.py:10:0: W0611: Unused Union imported from typing (unused-import)
client/backend/huggingface.py:10:0: W0611: Unused Tuple imported from typing (unused-import)
************* Module client.backend.ai_model
client/backend/ai_model.py:61:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:64:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:72:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:109:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:113:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:134:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:138:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:157:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:164:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:167:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:171:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:179:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:183:33: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:190:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:199:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:202:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:221:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:249:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:251:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:261:0: C0301: Line too long (106/100) (line-too-long)
client/backend/ai_model.py:274:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:277:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:292:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:295:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:311:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:315:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:321:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:341:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:344:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:349:0: C0301: Line too long (126/100) (line-too-long)
client/backend/ai_model.py:355:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:376:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:381:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:402:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:405:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:415:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:418:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:421:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:436:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:453:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:461:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:469:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:486:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:494:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:500:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:503:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/ai_model.py:39:0: C0103: Constant name "_gpt4all_model" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:40:0: C0103: Constant name "_openai" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:41:0: C0103: Constant name "_gemini" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:42:0: C0103: Constant name "_huggingface_pipeline" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:43:0: C0103: Constant name "_huggingface_tokenizer" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:44:0: C0103: Constant name "_huggingface_model" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:49:4: W0107: Unnecessary pass statement (unnecessary-pass)
client/backend/ai_model.py:58:4: W0603: Using the global statement (global-statement)
client/backend/ai_model.py:95:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:63:8: C0415: Import outside toplevel (gpt4all.GPT4All) (import-outside-toplevel)
client/backend/ai_model.py:70:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:71:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:76:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:85:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:91:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:96:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:69:12: W0612: Unused variable 'filename' (unused-variable)
client/backend/ai_model.py:106:4: W0603: Using the global statement (global-statement)
client/backend/ai_model.py:120:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:115:8: C0415: Import outside toplevel (openai) (import-outside-toplevel)
client/backend/ai_model.py:121:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:131:4: W0603: Using the global statement (global-statement)
client/backend/ai_model.py:145:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:140:8: C0415: Import outside toplevel (google.generativeai) (import-outside-toplevel)
client/backend/ai_model.py:146:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:156:4: W0603: Using the global statement (global-statement)
client/backend/ai_model.py:203:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:162:8: C0415: Import outside toplevel (transformers.AutoModelForCausalLM, transformers.AutoTokenizer, transformers.pipeline) (import-outside-toplevel)
client/backend/ai_model.py:163:8: C0415: Import outside toplevel (torch) (import-outside-toplevel)
client/backend/ai_model.py:170:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:173:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:181:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:204:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:208:14: W0621: Redefining name 'model_name' from outer scope (line 498) (redefined-outer-name)
client/backend/ai_model.py:222:4: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
client/backend/ai_model.py:261:31: W0621: Redefining name 'model_name' from outer scope (line 498) (redefined-outer-name)
client/backend/ai_model.py:273:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:289:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:290:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:294:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:313:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:322:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:325:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:343:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:346:19: E1101: Module 'openai' has no 'ChatCompletion' member (no-member)
client/backend/ai_model.py:357:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:360:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:378:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:383:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:386:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:404:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:422:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:425:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
client/backend/ai_model.py:399:4: W0612: Unused variable 'model' (unused-variable)
client/backend/ai_model.py:399:11: W0612: Unused variable 'tokenizer' (unused-variable)
client/backend/ai_model.py:435:4: W0621: Redefining name 'models' from outer scope (line 492) (redefined-outer-name)
client/backend/ai_model.py:446:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:479:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/ai_model.py:501:12: C0103: Constant name "test_log" doesn't conform to UPPER_CASE naming style (invalid-name)
client/backend/ai_model.py:10:0: W0611: Unused Union imported from typing (unused-import)
************* Module client.backend.gpt4allinit
client/backend/gpt4allinit.py:1:0: C0114: Missing module docstring (missing-module-docstring)
************* Module client.backend.hugging
client/backend/hugging.py:50:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/hugging.py:60:0: C0303: Trailing whitespace (trailing-whitespace)
client/backend/hugging.py:1:0: C0114: Missing module docstring (missing-module-docstring)
client/backend/hugging.py:3:0: E0611: No name 'AutoModelForTextGeneration' in module 'transformers' (no-name-in-module)
client/backend/hugging.py:11:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/hugging.py:21:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/hugging.py:31:11: W0718: Catching too general exception Exception (broad-exception-caught)
client/backend/hugging.py:34:0: C0116: Missing function or method docstring (missing-function-docstring)
client/backend/hugging.py:1:0: W0611: Unused import os (unused-import)
client/backend/hugging.py:3:0: W0611: Unused AutoModelForTextGeneration imported from transformers (unused-import)
client/backend/hugging.py:3:0: W0611: Unused AutoTokenizer imported from transformers (unused-import)
************* Module client.backend.gpt4all.app
client/backend/gpt4all/app.py:64:0: C0301: Line too long (121/100) (line-too-long)
client/backend/gpt4all/app.py:111:68: C0303: Trailing whitespace (trailing-whitespace)
client/backend/gpt4all/app.py:148:72: C0303: Trailing whitespace (trailing-whitespace)
client/backend/gpt4all/app.py:87:8: W0621: Redefining name 'version' from outer scope (line 178) (redefined-outer-name)
client/backend/gpt4all/app.py:91:4: W0702: No exception type(s) specified (bare-except)
************* Module client.frontend.twitchbot
client/frontend/twitchbot.py:1:0: C0114: Missing module docstring (missing-module-docstring)
client/frontend/twitchbot.py:4:0: C0115: Missing class docstring (missing-class-docstring)
client/frontend/twitchbot.py:23:4: C0116: Missing function or method docstring (missing-function-docstring)
client/frontend/twitchbot.py:2:0: C0411: standard import "os" should be placed before third party import "twitchio.ext.commands" (wrong-import-order)
************* Module client.frontend.config
client/frontend/config.py:1:0: C0114: Missing module docstring (missing-module-docstring)
client/frontend/config.py:14:0: W0622: Redefining built-in 'set' (redefined-builtin)
client/frontend/config.py:11:0: C0116: Missing function or method docstring (missing-function-docstring)
client/frontend/config.py:14:0: C0116: Missing function or method docstring (missing-function-docstring)
client/frontend/config.py:1:0: C0114: Missing module docstring (missing-module-docstring)
client/frontend/config.py:14:0: W0622: Redefining built-in 'set' (redefined-builtin)
client/frontend/config.py:11:0: C0116: Missing function or method docstring (missing-function-docstring)
client/frontend/config.py:14:0: C0116: Missing function or method docstring (missing-function-docstring)
************* Module server.backend.app
server/backend/app.py:53:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:59:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:64:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:70:0: C0301: Line too long (124/100) (line-too-long)
server/backend/app.py:71:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:74:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:107:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:112:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:116:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:118:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:147:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:161:0: C0301: Line too long (115/100) (line-too-long)
server/backend/app.py:170:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:175:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:179:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:211:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:233:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:242:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:252:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:256:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:260:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:277:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:286:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:289:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:294:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/app.py:25:11: W1508: os.getenv default type is builtins.int. Expected str or None. (invalid-envvar-default)
server/backend/app.py:89:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:69:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:80:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:90:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:119:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:120:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:132:8: W0621: Redefining name 'models' from outer scope (line 291) (redefined-outer-name)
server/backend/app.py:134:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:135:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:164:15: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:151:12: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
server/backend/app.py:165:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:182:15: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:180:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:183:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:218:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:212:8: R1705: Unnecessary "else" after "return", remove the "else" and de-indent the code inside it (no-else-return)
server/backend/app.py:219:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:261:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/app.py:262:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:287:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:288:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:293:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/app.py:11:0: W0611: Unused send_from_directory imported from flask (unused-import)
************* Module server.backend.ai_model
server/backend/ai_model.py:61:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:64:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:72:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:77:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:88:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:90:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:109:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:113:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:134:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:138:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:157:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:160:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:164:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:167:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:171:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:179:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:183:33: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:190:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:199:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:202:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:221:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:249:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:251:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:261:0: C0301: Line too long (106/100) (line-too-long)
server/backend/ai_model.py:274:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:277:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:292:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:295:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:311:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:315:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:321:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:341:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:344:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:349:0: C0301: Line too long (126/100) (line-too-long)
server/backend/ai_model.py:355:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:376:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:381:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:402:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:405:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:415:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:418:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:421:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:436:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:453:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:461:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:469:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:486:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:494:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:500:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:503:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/ai_model.py:39:0: C0103: Constant name "_gpt4all_model" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:40:0: C0103: Constant name "_openai" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:41:0: C0103: Constant name "_gemini" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:42:0: C0103: Constant name "_huggingface_pipeline" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:43:0: C0103: Constant name "_huggingface_tokenizer" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:44:0: C0103: Constant name "_huggingface_model" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:49:4: W0107: Unnecessary pass statement (unnecessary-pass)
server/backend/ai_model.py:58:4: W0603: Using the global statement (global-statement)
server/backend/ai_model.py:95:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:63:8: C0415: Import outside toplevel (gpt4all.GPT4All) (import-outside-toplevel)
server/backend/ai_model.py:70:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:71:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:76:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:85:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:91:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:96:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:69:12: W0612: Unused variable 'filename' (unused-variable)
server/backend/ai_model.py:106:4: W0603: Using the global statement (global-statement)
server/backend/ai_model.py:120:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:115:8: C0415: Import outside toplevel (openai) (import-outside-toplevel)
server/backend/ai_model.py:121:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:131:4: W0603: Using the global statement (global-statement)
server/backend/ai_model.py:145:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:140:8: C0415: Import outside toplevel (google.generativeai) (import-outside-toplevel)
server/backend/ai_model.py:146:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:156:4: W0603: Using the global statement (global-statement)
server/backend/ai_model.py:203:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:162:8: C0415: Import outside toplevel (transformers.AutoModelForCausalLM, transformers.AutoTokenizer, transformers.pipeline) (import-outside-toplevel)
server/backend/ai_model.py:163:8: C0415: Import outside toplevel (torch) (import-outside-toplevel)
server/backend/ai_model.py:170:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:173:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:181:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:204:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:208:14: W0621: Redefining name 'model_name' from outer scope (line 498) (redefined-outer-name)
server/backend/ai_model.py:222:4: R1705: Unnecessary "elif" after "return", remove the leading "el" from "elif" (no-else-return)
server/backend/ai_model.py:261:31: W0621: Redefining name 'model_name' from outer scope (line 498) (redefined-outer-name)
server/backend/ai_model.py:273:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:289:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:290:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:294:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:313:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:322:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:325:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:343:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:346:19: E1101: Module 'openai' has no 'ChatCompletion' member (no-member)
server/backend/ai_model.py:357:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:360:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:378:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:383:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:386:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:404:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:422:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:425:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/ai_model.py:399:4: W0612: Unused variable 'model' (unused-variable)
server/backend/ai_model.py:399:11: W0612: Unused variable 'tokenizer' (unused-variable)
server/backend/ai_model.py:435:4: W0621: Redefining name 'models' from outer scope (line 492) (redefined-outer-name)
server/backend/ai_model.py:446:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:479:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/ai_model.py:501:12: C0103: Constant name "test_log" doesn't conform to UPPER_CASE naming style (invalid-name)
server/backend/ai_model.py:10:0: W0611: Unused Union imported from typing (unused-import)
************* Module server.backend.websocketserv
server/backend/websocketserv.py:63:0: C0301: Line too long (103/100) (line-too-long)
server/backend/websocketserv.py:64:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:72:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:82:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:86:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:94:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:107:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:111:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:119:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:125:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:133:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:136:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:143:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:146:0: C0301: Line too long (101/100) (line-too-long)
server/backend/websocketserv.py:148:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:156:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:168:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:197:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:204:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:207:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:210:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:214:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:223:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:224:0: C0301: Line too long (104/100) (line-too-long)
server/backend/websocketserv.py:225:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:228:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:236:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:241:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:258:0: C0301: Line too long (103/100) (line-too-long)
server/backend/websocketserv.py:259:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:262:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:267:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:272:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:281:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:285:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:297:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:300:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:304:0: C0301: Line too long (102/100) (line-too-long)
server/backend/websocketserv.py:311:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:315:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:330:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:332:0: C0301: Line too long (104/100) (line-too-long)
server/backend/websocketserv.py:350:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:356:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:363:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:367:0: C0301: Line too long (112/100) (line-too-long)
server/backend/websocketserv.py:369:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:373:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:376:0: C0301: Line too long (129/100) (line-too-long)
server/backend/websocketserv.py:377:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:380:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:389:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:396:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:399:27: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:400:13: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:401:13: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:407:0: C0301: Line too long (117/100) (line-too-long)
server/backend/websocketserv.py:408:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:411:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:429:0: C0303: Trailing whitespace (trailing-whitespace)
server/backend/websocketserv.py:34:11: W1508: os.getenv default type is builtins.int. Expected str or None. (invalid-envvar-default)
server/backend/websocketserv.py:69:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:85:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:170:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:178:0: R0913: Too many arguments (6/5) (too-many-arguments)
server/backend/websocketserv.py:178:0: R0917: Too many positional arguments (6/5) (too-many-positional-arguments)
server/backend/websocketserv.py:226:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/websocketserv.py:224:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:227:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:333:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/websocketserv.py:261:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:274:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:278:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:290:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:323:19: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/websocketserv.py:304:20: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:317:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:324:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:332:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:334:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:342:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:249:0: R0912: Too many branches (14/12) (too-many-branches)
server/backend/websocketserv.py:249:0: R0915: Too many statements (53/50) (too-many-statements)
server/backend/websocketserv.py:249:40: W0613: Unused argument 'path' (unused-argument)
server/backend/websocketserv.py:378:15: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/websocketserv.py:358:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:376:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:379:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:387:4: W0601: Global variable 'server_start_time' undefined at the module level (global-variable-undefined)
server/backend/websocketserv.py:395:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:398:15: E1101: Module 'websockets' has no 'serve' member (no-member)
server/backend/websocketserv.py:407:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:410:8: W0612: Unused variable 'cleanup_task' (unused-variable)
server/backend/websocketserv.py:434:11: W0718: Catching too general exception Exception (broad-exception-caught)
server/backend/websocketserv.py:435:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)
server/backend/websocketserv.py:13:0: W0611: Unused Set imported from typing (unused-import)
server/backend/websocketserv.py:13:0: W0611: Unused Optional imported from typing (unused-import)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.backend.ai_model:[13:506]
==server.backend.ai_model:[13:506]
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    handlers=[
        logging.FileHandler("ai_model.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AIModel")

# Load environment variables
load_dotenv()

# Get API keys and model paths from environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY", "")
LLAMA_MODEL_PATH = os.getenv("LLAMA_MODEL_PATH", "Meta-Llama-3-8B-Instruct.Q4_0.gguf")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gpt4all")

# Hugging Face model configuration
HUGGINGFACE_MODEL_ID = os.getenv("HUGGINGFACE_MODEL_ID", "mistralai/Mistral-7B-Instruct-v0.2")
CACHE_DIR = os.getenv("HUGGINGFACE_CACHE_DIR", "./models/huggingface")

# Global model instances
_gpt4all_model = None
_openai = None
_gemini = None
_huggingface_pipeline = None
_huggingface_tokenizer = None
_huggingface_model = None


class ModelNotInitializedError(Exception):
    """Exception raised when a model cannot be initialized."""
    pass


def initialize_gpt4all():
    """Initialize the GPT4All model for offline processing.

    Returns:
        GPT4All model instance or None if initialization fails
    """
    global _gpt4all_model
    if _gpt4all_model is not None:
        return _gpt4all_model

    try:
        from gpt4all import GPT4All

        # Check if model exists
        model_path = os.path.expanduser(LLAMA_MODEL_PATH)
        if not os.path.exists(model_path):
            model_dir = os.path.dirname(model_path)
            filename = os.path.basename(model_path)
            logger.warning(f"Model file not found at: {model_path}")
            logger.info(f"Checking if model exists in directory: {model_dir}")

            # Check if the directory exists, create if not
            if not os.path.exists(model_dir):
                os.makedirs(model_dir, exist_ok=True)
                logger.info(f"Created model directory: {model_dir}")

            # List available models if directory exists
            if os.path.exists(model_dir):
                files = os.listdir(model_dir)
                gguf_files = [f for f in files if f.endswith('.gguf')]
                if gguf_files:
                    # Use the first available .gguf file
                    model_path = os.path.join(model_dir, gguf_files[0])
                    logger.info(f"Using available model: {model_path}")
                else:
                    logger.warning("No .gguf models found. Will download the default model.")

            # Model will be downloaded automatically by GPT4All if not found

        logger.info(f"Loading GPT4All model from: {model_path}")
        _gpt4all_model = GPT4All(model_path)
        logger.info("GPT4All model loaded successfully")
        return _gpt4all_model
    except Exception as e:
        logger.error(f"Error initializing GPT4All: {str(e)}")
        return None


def initialize_openai():
    """Initialize the OpenAI API client.

    Returns:
        OpenAI client object or None if initialization fails
    """
    global _openai
    if _openai is not None:
        return _openai

    if not OPENAI_API_KEY:
        logger.warning("No OpenAI API key found in environment")
        return None

    try:
        import openai
        openai.api_key = OPENAI_API_KEY
        _openai = openai
        logger.info("OpenAI API initialized successfully")
        return _openai
    except Exception as e:
        logger.error(f"Error initializing OpenAI API: {str(e)}")
        return None


def initialize_gemini():
    """Initialize the Google Gemini API client.

    Returns:
        Gemini client object or None if initialization fails
    """
    global _gemini
    if _gemini is not None:
        return _gemini

    if not GEMINI_API_KEY:
        logger.warning("No Gemini API key found in environment")
        return None

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        _gemini = genai
        logger.info("Google Gemini API initialized successfully")
        return _gemini
    except Exception as e:
        logger.error(f"Error initializing Gemini API: {str(e)}")
        return None


def initialize_huggingface():
    """Initialize the Hugging Face model for inference.

    Returns:
        A tuple of (model, tokenizer, pipeline) or None if initialization fails
    """
    global _huggingface_model, _huggingface_tokenizer, _huggingface_pipeline

    if _huggingface_pipeline is not None:
        return _huggingface_model, _huggingface_tokenizer, _huggingface_pipeline

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
        import torch

        # Create cache directory if it doesn't exist
        os.makedirs(CACHE_DIR, exist_ok=True)

        # Check for CUDA availability
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device} for Hugging Face model")

        # Load tokenizer first
        logger.info(f"Loading Hugging Face tokenizer: {HUGGINGFACE_MODEL_ID}")
        _huggingface_tokenizer = AutoTokenizer.from_pretrained(
            HUGGINGFACE_MODEL_ID,
            cache_dir=CACHE_DIR,
            token=HUGGINGFACE_API_KEY if HUGGINGFACE_API_KEY else None
        )

        # Load model with appropriate configuration
        logger.info(f"Loading Hugging Face model: {HUGGINGFACE_MODEL_ID}")
        _huggingface_model = AutoModelForCausalLM.from_pretrained(
            HUGGINGFACE_MODEL_ID,
            cache_dir=CACHE_DIR,
            token=HUGGINGFACE_API_KEY if HUGGINGFACE_API_KEY else None,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            low_cpu_mem_usage=True,
            device_map="auto" if device == "cuda" else None
        )

        # Create text generation pipeline
        logger.info("Creating Hugging Face pipeline")
        _huggingface_pipeline = pipeline(
            "text-generation",
            model=_huggingface_model,
            tokenizer=_huggingface_tokenizer,
            device=0 if device == "cuda" else -1
        )

        logger.info("Hugging Face model initialized successfully")
        return _huggingface_model, _huggingface_tokenizer, _huggingface_pipeline

    except Exception as e:
        logger.error(f"Error initializing Hugging Face model: {str(e)}")
        return None, None, None


def get_model(model_name: str):
    """Get the requested AI model.

    Args:
        model_name: Name of the model to use ('gpt4all', 'openai', 'gemini', 'huggingface')

    Returns:
        Model instance or None if initialization fails

    Raises:
        ValueError: If an unknown model name is provided
    """
    model_name = model_name.lower()

    if model_name == "gpt4all":
        return initialize_gpt4all()
    elif model_name == "openai":
        return initialize_openai()
    elif model_name == "gemini":
        return initialize_gemini()
    elif model_name == "huggingface":
        return initialize_huggingface()
    else:
        raise ValueError(f"Unknown model: {model_name}")


def create_prompt(log_text: str, instruction: Optional[str] = None) -> str:
    """Create a standardized prompt for log analysis.

    Args:
        log_text: The log text to analyze
        instruction: Optional specific instruction to override default

    Returns:
        Formatted prompt string
    """
    default_instruction = """Analyze the following log and provide insights:
1. Summarize what the log is showing
2. Identify any errors or warnings
3. Suggest potential solutions if problems are found
"""

    instruction = instruction or default_instruction

    return f"""{instruction}

LOG:
{log_text}

ANALYSIS:
"""


def analyze_log(log_text: str, model_name: str = DEFAULT_MODEL, instruction: Optional[str] = None) -> str:
    """Analyze log text using the specified AI model.

    Args:
        log_text: The log text to analyze
        model_name: Name of the model to use for analysis
        instruction: Optional specific instruction for the model

    Returns:
        Analysis result as a string
    """
    start_time = time.time()
    logger.info(f"Analyzing log with model: {model_name}")

    # Create the prompt
    prompt = create_prompt(log_text, instruction)

    try:
        if model_name == "gpt4all":
            response = gpt4all_response(prompt)
        elif model_name == "openai":
            response = openai_response(prompt)
        elif model_name == "gemini":
            response = gemini_response(prompt)
        elif model_name == "huggingface":
            response = huggingface_response(prompt)
        else:
            return f" Error: Unknown model '{model_name}' specified"
    except Exception as e:
        logger.exception(f"Error analyzing log with {model_name}: {str(e)}")
        return f" Error analyzing log: {str(e)}"

    elapsed_time = time.time() - start_time
    logger.info(f"Log analysis completed in {elapsed_time:.2f} seconds")

    return response


def gpt4all_response(prompt: str) -> str:
    """Get a response from the GPT4All model.

    Args:
        prompt: The prompt to send to the model

    Returns:
        Model response as a string
    """
    model = initialize_gpt4all()
    if not model:
        raise ModelNotInitializedError("GPT4All model could not be initialized")

    try:
        logger.debug(f"Sending prompt to GPT4All (length: {len(prompt)})")
        response = ""

        # Use with context for proper resource handling
        with model.chat_session():
            # Stream response tokens for better performance monitoring
            for token in model.generate(prompt, max_tokens=2048, temp=0.7):
                response += token

        logger.debug(f"Received GPT4All response (length: {len(response)})")
        return response.strip()
    except Exception as e:
        logger.exception(f"Error with GPT4All: {str(e)}")
        raise


def openai_response(prompt: str) -> str:
    """Get a response from the OpenAI API.

    Args:
        prompt: The prompt to send to the model

    Returns:
        Model response as a string
    """
    openai = initialize_openai()
    if not openai:
        raise ModelNotInitializedError("OpenAI API could not be initialized. Check your API key.")

    try:
        logger.debug(f"Sending prompt to OpenAI (length: {len(prompt)})")

        # Use the ChatCompletion API
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a log analysis expert. Provide clear, concise analysis of log files."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1024,
            temperature=0.5
        )

        response_text = response["choices"][0]["message"]["content"].strip()
        logger.debug(f"Received OpenAI response (length: {len(response_text)})")
        return response_text
    except Exception as e:
        logger.exception(f"Error with OpenAI: {str(e)}")
        raise


def gemini_response(prompt: str) -> str:
    """Get a response from the Google Gemini API.

    Args:
        prompt: The prompt to send to the model

    Returns:
        Model response as a string
    """
    gemini = initialize_gemini()
    if not gemini:
        raise ModelNotInitializedError("Gemini API could not be initialized. Check your API key.")

    try:
        logger.debug(f"Sending prompt to Gemini (length: {len(prompt)})")
        model = gemini.GenerativeModel('gemini-pro')
        response = model.generate_content(prompt)

        response_text = response.text
        logger.debug(f"Received Gemini response (length: {len(response_text)})")
        return response_text.strip()
    except Exception as e:
        logger.exception(f"Error with Gemini: {str(e)}")
        raise


def huggingface_response(prompt: str) -> str:
    """Get a response from the Hugging Face model.

    Args:
        prompt: The prompt to send to the model

    Returns:
        Model response as a string
    """
    model, tokenizer, pipe = initialize_huggingface()
    if not pipe:
        raise ModelNotInitializedError("Hugging Face model could not be initialized.")

    try:
        logger.debug(f"Sending prompt to Hugging Face (length: {len(prompt)})")

        # Generate text with appropriate parameters
        outputs = pipe(
            prompt,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.15,
            do_sample=True
        )

        # Extract the generated text
        generated_text = outputs[0]['generated_text']

        # Remove the prompt from the response
        response_text = generated_text[len(prompt):].strip()

        logger.debug(f"Received Hugging Face response (length: {len(response_text)})")
        return response_text
    except Exception as e:
        logger.exception(f"Error with Hugging Face: {str(e)}")
        raise


def get_available_models() -> List[Dict[str, Any]]:
    """Get information about available models.

    Returns:
        List of dictionaries with model information
    """
    models = []

    # Check GPT4All
    try:
        gpt4all = initialize_gpt4all()
        models.append({
            "name": "gpt4all",
            "available": gpt4all is not None,
            "type": "local",
            "file": LLAMA_MODEL_PATH
        })
    except Exception:
        models.append({
            "name": "gpt4all",
            "available": False,
            "type": "local",
            "error": "Failed to initialize"
        })

    # Check OpenAI
    models.append({
        "name": "openai",
        "available": OPENAI_API_KEY != "",
        "type": "api",
        "model": "gpt-4"
    })

    # Check Gemini
    models.append({
        "name": "gemini",
        "available": GEMINI_API_KEY != "",
        "type": "api",
        "model": "gemini-pro"
    })

    # Check Hugging Face
    try:
        _, _, pipe = initialize_huggingface()
        models.append({
            "name": "huggingface",
            "available": pipe is not None,
            "type": "local" if not HUGGINGFACE_API_KEY else "api",
            "model": HUGGINGFACE_MODEL_ID
        })
    except Exception:
        models.append({
            "name": "huggingface",
            "available": False,
            "type": "local",
            "error": "Failed to initialize"
        })

    return models


if __name__ == "__main__":
    # Simple test for the module
    models = get_available_models()
    print(json.dumps(models, indent=2))

    # Test a model if available
    for model_info in models:
        if model_info["available"]:
            model_name = model_info["name"]
            print(f"\nTesting {model_name} model...")

            test_log = "2023-05-01 12:34:56 ERROR Failed to connect to database: Connection refused"
            result = analyze_log(test_log, model_name)

            print(f"\nAnalysis result from {model_name}:")
            print(result)
            break (duplicate-code)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.backend.app:[17:36]
==server.backend.app:[16:35]
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Server configuration with fallback values
HOST = os.getenv("FLASK_HOST", "0.0.0.0")  # Default to all interfaces
PORT = int(os.getenv("FLASK_PORT", 8081))   # Default to 8081
DEBUG = os.getenv("FLASK_DEBUG", "False").lower() == "true"
ENV = os.getenv("ENVIRONMENT", "development")

# Configure logging
log_directory = os.path.join(os.path.dirname(__file__), "logs")
os.makedirs(log_directory, exist_ok=True)
log_file_path = os.path.join(log_directory, "backend.log")

logging.basicConfig(
    level=logging.DEBUG if DEBUG else logging.INFO, (duplicate-code)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.backend.app:[110:130]
==server.backend.app:[114:134]
            return jsonify({"logs": logs})

        return jsonify({"logs": []})

    except Exception as e:
        logger.exception(f"Error retrieving logs: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/models', methods=['GET'])
def get_models():
    """Get information about available AI models.

    Returns:
        JSON response with model information
    """
    try:
        models = get_available_models()
        return jsonify({"models": models})
    except Exception as e: (duplicate-code)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.frontend.config:[3:15]
==client.frontend.config:[3:15]
CONFIG = {
    "logging_enabled": True,
    "use_gpu_only": False,
    "synchronous_mode": False,
    "chatgpt_enabled": bool(os.getenv("OPENAI_API_KEY"))
}

def get(key):
    return CONFIG.get(key)

def set(key, value):
    CONFIG[key] = value (duplicate-code)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.backend.app:[60:70]
==server.backend.app:[53:62]
    try:
        # Validate input data
        if not request.is_json:
            logger.error("Request does not contain valid JSON")
            return jsonify({"error": "Request must be in JSON format"}), 400

        data = request.json
        log_text = data.get('log')
        model_name = data.get('model', 'gpt4all')  # Default to gpt4all
 (duplicate-code)
server/backend/websocketserv.py:1:0: R0801: Similar lines in 2 files
==client.backend.app:[208:222]
==server.backend.app:[183:197]
            return jsonify({"error": str(e)}), 500


@app.route('/health', methods=['GET'])
def health_check():
    """Check the health of the backend server.

    Returns:
        JSON response with server status
    """
    return jsonify({
        "status": "online",
        "environment": ENV,
        "version": "1.0.0", (duplicate-code)

------------------------------------------------------------------
Your code has been rated at 5.23/10 (previous run: 5.23/10, +0.00)

